---
title: "Projekt"
date: "20 nov 2019"
output: html_document
---
# Comparing trained and pretrained models on Image Classification for multiple classes

## Image classification for multiple classes

In this project, we are focusing on dealing with image classification, which refers to a process in computer vision that can classify an image according to its visual content. Over the last years, it has grown effective due to it using deep learning. Deep learning is a class of machine learning that uses multiple layers to extract higher level features. Some expamples on its usage are Face Recognition on social platforms like Facebook or Product Discoverability which allows consumers to search for similar images or products.  

Keras has 10 pretrained models, for example VGG19 and ResNet50, which are trained on ImageNet, a large collection of images in 1000 classes, and the pretrained models should be able to classify an image that falls into any of these 1000 classes. An image classifier contains convolutional and classifier layers. The convolutional layers extract features and classifier layers classify them using these features. Pretrained models already know how to extract features, and therefore you don't need to train it from scratch. 

We have choosen to work with one of the simpler pretrained models, here the VGG19 model, but you can easily use different models from the Keras library as the procedure is the same. The VGG19 model is a CNN that is trained on more than one million images from ImageNet. It is 19 layers deep.

## The data set
In this project we're making a guide on image classification in R. We're working with the Fruit data set from Kaggle, found on https://www.kaggle.com/moltean/fruits/data. Here 120 different fruits and vegetables are stored in different classes. The data set has 82213 images of fruit and vegetables. The data set is split into a training set containing 61488 images and a test set containing 20622 images. The images sizes are 100x100 pixels. 

In the data set there was a difference in lighting conditions, the background was not uniform and beforehand there has been written a dedicated algorithm which extracts the fruit from the background, so the fruit and vegetables are seen on the images with a white background.

## Comparing CNN and VGG19 for multiple classes

## An example using CNN

### Load the data
We start by cleaning the global environment. 

```{r}
#Cleaning the environment
rm(list=ls())
```

And then we'll import Keras, which is essential for the anaysis. You have to use "install_keras" if you're installing Keras for the first time.

```{r}
devtools::install_github("rstudio/keras", force = TRUE)
library(keras)
#install_keras()
```

And then we'll load a bunch of other packages.

```{r, include=FALSE}
#Loading packages
if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
pacman::p_load(knitr, # For knitr to html
               rmarkdown, # For formatting the document
               tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               data.table, # for reading in data ect. 
               magrittr,# For advanced piping (%>% et al.)
               igraph, # For network analysis
               tidygraph, # For tidy-style graph manipulation
               ggraph, # For ggplot2 style graph plotting
               Matrix, # For some matrix functionality
               ggforce, # Awesome plotting
               kableExtra, # Formatting for tables
               car, # recode functions 
               tidytext, # Structure text within tidyverse
               topicmodels, # For topic modelling
               tm, # text mining library
               quanteda, # for LSA (latent semantic analysis)
               uwot, # for UMAP
               dbscan, # for density based clustering
               SnowballC,
               textdata,
               wordcloud, 
               textstem, # for textstemming 
               tidyr,
               widyr,
               reshape2,
               quanteda,
               uwot,
               dbscan,
               plotly,
               rsample,
               glmnet,
               broom,
               yardstick,
               lda, # For LDA-analysis
               topicmodels, # LDA models
               broom,
               keras,
               drat,
               reticulate,
               BiocManager
               )
```


### Preprocessing 

And then we can load the data the data from Kaggle. We just loaded it locally, downloading and unpacking it from Kaggle.

```{r}
train_images = "/Users/Emma/OneDrive/Universitetet/9. semester/SDS/M3/fruits-360_dataset/fruits-360/Training/"

test_images = "/Users/Emma/OneDrive/Universitetet/9. semester/SDS/M3/fruits-360_dataset/fruits-360/Test/"
```

```{r}
train_images = "C:/Users/Michael Thomsen/Desktop/fruits-360_dataset/fruits-360/Training/"

test_images = "C:/Users/Michael Thomsen/Desktop/fruits-360_dataset/fruits-360/Test/"
```


Then we need to define a vector of the fruits and vegetables we want to train our model to classify, as we're not going to train it to classify 120. At the same time we also define some variabels to scale down the picture - which original is 100 x 100 pixels.

```{r}
FV_list <- c("Quince","Tangelo","Cactus fruit","Physalis","Pepino","Orange","Blueberry","Mulberry",
             "Kaki","Guava","Eggplant","Carambula","Beetroot","Apricot","Avocado", "Banana")

length <- length(FV_list)
```

Then we're using the function "image_data_generator", which can load and generate batches of the image data. Here we're first rescaling the image data. For most image data, the pixel values are integers with values between 0 and 255, which is why dividing the data with 255. This is performed across all channels, regardless of the actual range of pixel values that are present in the images. Here we're also applying zoom range, which is the amount of zoom. We're also rescaling the test data set

```{r}
train_images_rescale = image_data_generator(rescale = 1/255)

test_images_rescale = image_data_generator(rescale = 1/255)
```

We now want to load our images into the memory and resize them. To do this the 'flow_images_from_directory' function from the Keras package will come in use. It allows us to generate batches of data from images in a directory. We start by setting the 'class_mode' to categorical, due to the data list we created earlier. The same applies for the which 'classes', we are refering back to. At last we the set the seed to 123, as it allows us to reproduce the same results later.


```{r}
train_image_array <- flow_images_from_directory(train_images, 
                                                    train_images_rescale,
                                                    target_size = c(64,64),
                                                    class_mode = "categorical",
                                                    classes = FV_list,
                                                    seed = 31)


test_image_array <- flow_images_from_directory(test_images, 
                                                    test_images_rescale,
                                                    target_size = c(64,64),
                                                    class_mode = "categorical",
                                                    classes = FV_list,
                                                    seed = 31)  
```

We can now explore how many images we have in each of our list of fruits.

```{r}
cat("Number of images per class:")
table(factor(train_image_array$classes))
```

As the table show, there is a nice distrubtion of number of images in every classes. That's good so we dont have a group of classes which is over represented over others classes.


# Defining the model

Now it's time to set up the model, that should be able to predict the images. To start of we will define the number of training and testing samples, aswell as epochs.
##Epochs
The number of epochs for training indicates, which amount of times the the model will expose itself to the whole training set.

We are now ready for creating a model. We'll start of with a simple Convolutional Neural Net (CNN) model. It will contain the following layers: 2 Convolutional, 1 Pooling and 1 Dense.
The first thing is to use the 'keras_model_sequential' function which allows us to composing a model with diffrent kind of linear layers.
In the last layer we use the 'layer_dropout', which is a technique that improves the model with over-fit on neural networks. By using it the  classification error in the model will decrease.

```{r}
model <- keras_model_sequential()


model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same", input_shape = c(64,64, 3), activation = "relu") %>%
  layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = "same") %>%
  layer_activation_leaky_relu(0.5) %>%
  layer_batch_normalization() %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_flatten() %>%
  layer_dense(100, activation = "relu") %>%
  layer_dropout(0.5) %>%
  layer_dense(length, activation = "softmax")

```

Now it's time to compile the model. Which make sure we can configurate the model for the training. 
We're choosing the metrics is set to 'accuracy' so we can measure the perfomance of the model.

```{r}
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam",
  metrics = "accuracy"
)
```

Because we used image_data_generator() and flow_images_from_directory() earlier. We need to use the fit_generator(), when fitting the model on our training data. This function is similiar to the function 'fit()', which require a epochs and batch_size. 

```{r}
hist <- model %>% fit_generator(train_image_array,
                                steps_per_epoch = 100, 
                                epochs = 10, 
                                validation_data = test_image_array,
                                validation_steps = 100
)
```


```{r}
mean(hist$metrics$accuracy)
mean(hist$metrics$val_accuracy)
```

As shown above the model performed really well. Where the average accuracy is found to be 81%, which is quit high and adequate results. Some reasons to why the result is so high, could depend on pictures format, where the fruit is placed on a white background. So the model doesn't get any noise from the background of the picture. If we put this against the validation accuracy, where is slighty 1% higher in validation.


# Pretrained model
In this example, we would use a pretrained model to see how it compares to our own model. 

## Setting up your own pretrained model from Keras

1. Choose pretrained model and load it using "application_"
2. Choose if you want to freeze any layers
3. Trainsform model into sequational or xxx data
4. Add layers if wanted
5. Choose optimizer, loss and metrics
6. Run model using "fit_generator"

## An example

You have easily access to pretrained model, in R you simply have to write "application_" and then 10 pretrained models will show, amongst other models also the VGG16 model.

We're using a sequential model, which is a linear stack of layers. First we start by loading the pretrained model. Here there's a lot of options for the model. Here we can apply weights to the ImageNet images. 

```{r}
vgg19 = keras::application_vgg19(include_top = FALSE,
                                       weights = 'imagenet', 
                                       input_shape = c(64, 64 ,3))
```

Then you have the possibility to freeze layers. When a layer is frozen, its weights are frozen as well while training. If your current dataset is similar to the one it was freezed on, then it's good to freeze it, otherwise you can train the bottom layers. Here

```{r}
resnet501 = resnet50
resnet501 %>% freeze_weights(from = 1, to = 20)
```

Then 
```{r}
summary(resnet50)
```

```{r}
model = keras_model_sequential(resnet501)
model1 = keras_model_sequential(resnet501)
```

```{r}
model1 = model %>% layer_flatten() %>% layer_dense(units = 1024, activation = 'relu') %>% layer_dropout(0.5)
model2 = model1 %>% layer_dense(units = 16, activation = "softmax")
```

```{r}
summary(model2)
```


```{r}
model2 %>% compile(
  optimizer = "adam",
  loss = "categorical_crossentropy",
  metrics = "accuracy"
)
```

Now we run the model. 

```{r}
history = model2 %>% fit_generator(
  train_image_array,
  epochs = 5,
  steps_per_epoch = 50,
  validation_data = test_image_array,
  validation_steps = 50
)
```

```{r}
history$metrics
```

```{r}
mean(history$metrics$accuracy)
mean(history$metrics$val_accuracy)
```


Here the valuation accuracy is between 0.90 and 0.94, which is quiet high and higher than our own trained model.

## Why does the pretrained models perform better?

Pretrained models do not necessarily perform better than selftrained model, but they do have an advantage. Pretrained models are a bit like looking at two people, who both have never played football in their life, but one is an athlete and one isn't. As said, none of them know football beforehand, but the athlete does have an advantage and may be better, as he has strength and stamina and is in shape. Pretrained models are already trained 

We also tested the ResNet50 model, but it performed a lot worse than out own model, therefore it is not certain that it will perform better. 

## What have we learned?
We have learned that simple selftrained model like the CNN model we created in the start. Not could outperform the pretrained model. This could occur from the complexity of the model, which may detect more classifications error then the simple model. None the less we actually saw that the pretrained model gets an higher accuracy on around 10 percentage points, then the selftrained CNN model.
This is an exiciting topic to dig further in. Due to the pretrained model seems to be more used as a standard procedure.
And considering that where DL is being used in industries such as. health, finance and retail, etc. Where the impact of a little increase in the accuracy could make big difference.


